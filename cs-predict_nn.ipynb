{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(xy_filepath):\n",
    "    xy = np.genfromtxt(xy_filepath, delimiter=',')\n",
    "    return xy[:, :-1].astype(np.float), xy[:, -1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, X, y, transformations=None):\n",
    "        self.len = len(X)\n",
    "        self.x_data = torch.from_numpy(X).float()\n",
    "        self.y_data = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into global variable space\n",
    "# split between test and train sets\n",
    "FRAC_TRAIN = 0.8\n",
    "X, y = extract_data('./Xy.txt')\n",
    "num_train = int(FRAC_TRAIN * len(X))\n",
    "indices = np.random.permutation(len(X))\n",
    "train_indices, test_indices = indices[:num_train], indices[num_train:]\n",
    "X_train = X[train_indices, :]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices, :]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=True)\n",
    "test_dataset = Dataset(X_test, y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=128,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "#         self.fc3 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "#         x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latest_experiment():\n",
    "    epoch_number = np.arange(0, max_epochs, 1)\n",
    "\n",
    "    # Plot the loss over epoch\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_number, train_losses)\n",
    "    plt.title('loss over epochs')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the training accuracy over epoch\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_number, train_accuracies)\n",
    "    plt.title('training accuracy over epochs')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the test accuracy over epoch\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_number, test_accuracies)\n",
    "    plt.title('test accuracy over epochs')\n",
    "    plt.xlabel('Number of Epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(neural_network, train_loader, test_loader, loss_function, optimizer):\n",
    "    \"\"\"\n",
    "    this function is copy-pasted from hw3. look there for more info\n",
    "    Returns tuple: (testing accuracy, training accuracy, training loss), e.g. (0.76, 0.24, 0.56)\n",
    "    \"\"\"\n",
    "    # first pass: train the model\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Get inputs and labels from data loader \n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # predict, forward, backwards, optimize (update weights)\n",
    "        y_pred = net(inputs)\n",
    "        loss = loss_function(y_pred, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # second pass: calculate stats\n",
    "    train_total_correct = 0\n",
    "    train_total_loss = 0\n",
    "    train_total_instances = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Get inputs and labels from data loader \n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        num_instances = len(labels)\n",
    "        \n",
    "        # predict\n",
    "        y_pred = net(inputs)\n",
    "        loss = loss_function(y_pred, labels.view(-1, 1))\n",
    "        train_total_loss += num_instances * loss.data[0]\n",
    "        \n",
    "        # calculate the training accuracy\n",
    "        pred_np = np.round(y_pred.data.cpu().numpy().reshape(len(labels), 1)).astype(int)\n",
    "        label_np = labels.data.cpu().numpy().reshape(len(labels), 1)\n",
    "        \n",
    "#         print(pred_np).astype(int)\n",
    "#         print(label_np)\n",
    "#         return\n",
    "        \n",
    "        for j in range(pred_np.shape[0]):\n",
    "            if pred_np[j,:] == label_np[j,:]:\n",
    "                train_total_correct += 1\n",
    "        train_total_instances += num_instances\n",
    "        \n",
    "    train_accuracy = float(train_total_correct) / float(train_total_instances)\n",
    "    train_loss = float(train_total_loss) / float(train_total_instances)\n",
    "    \n",
    "    # pass for test data: calculate stats\n",
    "    test_total_correct = 0\n",
    "    test_total_instances = 0\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        # Get inputs and labels from data loader \n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        num_instances = len(labels)\n",
    "        \n",
    "        # predict\n",
    "        y_pred = net(inputs)\n",
    "        loss = loss_function(y_pred, labels.view(-1, 1))\n",
    "        \n",
    "        # calculate the test accuracy\n",
    "        pred_np = np.round(y_pred.data.cpu().numpy().reshape(len(labels), 1)).astype(int)\n",
    "        label_np = labels.data.cpu().numpy().reshape(len(labels), 1)\n",
    "\n",
    "        for j in range(pred_np.shape[0]):\n",
    "            if pred_np[j,:] == label_np[j,:]:\n",
    "                test_total_correct += 1\n",
    "        test_total_instances += num_instances\n",
    "        \n",
    "    test_accuracy = float(test_total_correct) / float(test_total_instances)\n",
    "\n",
    "    return (test_accuracy, train_accuracy, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()\n",
    "net.cuda()\n",
    "\n",
    "max_epochs = 150\n",
    "train_losses = np.zeros((max_epochs))\n",
    "train_accuracies = np.zeros((max_epochs))\n",
    "test_accuracies = np.zeros((max_epochs))\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "for epoch in range(max_epochs):\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"epoch:\", epoch)\n",
    "    test_accuracy, train_accuracy, train_loss = run_experiment(\n",
    "        net,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        optimizer\n",
    "    )\n",
    "    train_losses[epoch] = train_loss\n",
    "    train_accuracies[epoch] = train_accuracy\n",
    "    test_accuracies[epoch] = test_accuracy\n",
    "\n",
    "print(\"final train accuracy:\", train_accuracies[max_epochs - 1])\n",
    "print(\"final train loss:\", train_losses[max_epochs - 1])\n",
    "print(\"final test accuracy:\", test_accuracies[max_epochs - 1])\n",
    "plot_latest_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latest_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
